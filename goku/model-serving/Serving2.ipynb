{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e583ce-0066-44fe-be27-ef251c0ad842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Define the path to your local model\n",
    "model_path = '../goku/mlbakery'\n",
    "\n",
    "# Load the model using ctransformers\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, model_file=\"Phi-3-mini-4k-instruct-Q4.gguf\", gpu_layers=0)\n",
    "prompt = 'Who is the CEO of Google?'\n",
    "for token in model(prompts, stream=True, threads=4, max_new_tokens=512, stop=['<|end|>']):\n",
    "    yield (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9fbde-c1c2-4310-9fcb-d3ba77c44fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the path to the current working directory\n",
    "cwd = os.getcwd()\n",
    "save_path = 'work_dir'\n",
    "\n",
    "# Set the TRANSFORMERS_CACHE environment variable to the desired local path\n",
    "os.environ['TRANSFORMERS_CACHE'] = cwd\n",
    "\n",
    "# Specify the model you want to download\n",
    "model_name = 'all-MiniLM-L6-v2'  # You can choose any model available in sentence-transformers\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "model.save(save_path + '/' + model_name)\n",
    "\n",
    "# The model will be saved in the current working directory as specified by the TRANSFORMERS_CACHE variable\n",
    "print(f\"Model '{model_name}' downloaded and saved to {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fe840-f3c0-4d14-95d8-c0db4d938bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our LLM\n",
    "Settings.llm = None\n",
    "\n",
    "# define embed model\n",
    "Settings.embed_model = \"local:work_dir/all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "# Load the your data\n",
    "documents = SimpleDirectoryReader(\"../goku/dream/data/\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Query and print response\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Impact of genetics on camelid health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba818-c042-4f91-8b50-dd44680a6887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context = response.source_nodes[0].text[:800]\n",
    "prompt = 'What is the impact of genetics on camelid health?'\n",
    "\n",
    "combined_prompt = f\"<|user|>\\n{prompt}. Context information is below: {context}<|end|>\\n<|assistant|>\"\n",
    "for token in model(combined_prompt, stream=True, threads=4, max_new_tokens=512, stop=['<|end|>']):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991aa7ab-177b-4393-9b5b-2bdf1d6466ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    n: int\n",
    "    model: Any\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        combined_prompt = f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\"\n",
    "        return model(combined_prompt, threads=4, max_new_tokens=512, stop=['<|end|>'])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        This method should be overridden by subclasses that support streaming.\n",
    "\n",
    "        If not implemented, the default behavior of calls to stream will be to\n",
    "        fallback to the non-streaming version of the model and return\n",
    "        the output as a single chunk.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        combined_prompt = f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\"\n",
    "        for token in model(combined_prompt, stream=True, threads=4, max_new_tokens=512, stop=['<|end|>']):\n",
    "            chunk = GenerationChunk(text=token)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7d09f-335f-4303-9ac6-af3bef28d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = CustomLLM(n=2, model=model)\n",
    "\n",
    "custom.astream('Who is the CEO of DBS?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7776d49-6b60-48ef-a86b-e25acf794ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async for token in custom.astream(\"Who is the CEO of DBS?\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c86e8-379f-449b-ba8a-ec3bf4a64f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install garak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0f171-b9dc-4a0e-8814-31206c9c20ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install giskard[llm] -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95112c58-b628-4200-bc9b-4bad398099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard import Model, Dataset, scan, GiskardClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd64988-173c-4edf-b7b0-fe0b7274adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import giskard\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def model_predict(df: pd.DataFrame):\n",
    "    \"\"\"Wraps the LLM call in a simple Python function.\n",
    "\n",
    "    The function takes a pandas.DataFrame containing the input variables needed\n",
    "    by your model, and must return a list of the outputs (one for each row).\n",
    "    \"\"\"\n",
    "    return [custom(question) for question in df[\"question\"]]\n",
    "\n",
    "\n",
    "# Donâ€™t forget to fill the `name` and `description`: they are used by Giskard\n",
    "# to generate domain-specific tests.\n",
    "giskard_model = giskard.Model(\n",
    "    model=model_predict,\n",
    "    model_type=\"text_generation\",\n",
    "    name=\"Climate Change Question Answering\",\n",
    "    description=\"This model answers any question about climate change based on IPCC reports\",\n",
    "    feature_names=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c482e96-7187-4a22-9196-dcb55524b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "giskard_dataset = Dataset(pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"According to the IPCC report, what are key risks in the Europe?\",\n",
    "        \"Is sea level rise avoidable? When will it stop?\"\n",
    "    ]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd712096-8884-4ba1-9b0d-a7b4a9534f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "giskard.llm.set_llm_model = custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fd4ad-a916-457a-9eb0-451d20ab2d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(giskard_model.predict(giskard_dataset).prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ee473-5d58-49b2-88fd-547785e9085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = giskard.scan(giskard_model, giskard_dataset, only=\"hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49698a7-abda-4c83-b4e9-b48a3978f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f791c-9253-43af-a728-4f1437ea25ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
